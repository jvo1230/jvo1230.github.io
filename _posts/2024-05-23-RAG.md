---
title: "RAG to Riches: How to Give Your Chatbot a Memory Upgrade"
tags: [Retrieval Augmented Generation, LLMs, ChatBots, OpenAI, Code]
style: fill
color: primary
description: What Retrieval Augmented Generation is, and how to implement it in a chat bot.
---
![Firstimage](https://cdn.prod.website-files.com/5e42772e6a8cfd42a9715206/659bb6465301acb6d7c94673_Feature_Chatbot-Examples-Article-Update%20(1).jpg)
<br><b>ChatGPT.</b> 

We've probably all used it once. Maybe to find recipes, answer questions, or to write our essays for us. ChatGPT is an example of a Large Language model (LLM). These LLMs are trained on a massive amount of text and are able to simulate human conversation and answer specific questions. But of course, sometimes these LLMs can't answer everything. For example, what if we wanted to ask ChatGPT a question about current stock levels for a store? Since that information wasn't a part of it's training data, theres no way for it to answer. More often than not, we'll often encounter situations where ChatGPT isn't able to answer our question and will even make up and hallucinate an answer! 

{% include elements/figure.html image="https://github.com/user-attachments/assets/36168adc-3051-4abd-b329-f84014365bb6" caption="ChatGPT Hallucinating" %}

But fear not! There's still a way we can use LLM's to help us answer specific questions it doesn't know. Retrieval Augmented Generation (RAG).

<h2>What is Retrieval Augmented Generation?</h2>
Retrieval Augmented Generation, or RAG, is a technique which combines LLMs with external knowledge to help it answer questions it doesn't know. It involves two key steps, retrieving relevant information from a source, such as a database, based on a users query, and then feeding that information, along with the original query, to an LLM to generate an output.

<figure style="text-align: center;">
  <!-- Image -->
  <img src="https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1698862153-image4.png&w=3840&q=75" alt="Description of the image" style="width:100%">

  <!-- Caption with a hyperlink -->
  <figcaption>
    <a href="https://deepgram.com/ai-glossary/retrieval-augmented-generation" target="_blank"><u>RAG Architecture</u></a>
  </figcaption>
</figure>


The way RAG retrieves relevant information is by first converting the user's query into vector embeddings. These embeddings are then searched through a vector database to find the relevant documents. A vector database consists of precomputed vector embeddings of the contexual information. It allows for efficient similarity searches by comparing the vectors it stores, and enabling tasks like document retrieval. For example, if a user inputed the query "How many apples are left in the store", the query's vector embedding would be searched in a vector database storing information about stock levels. Relevant documents would then be returned based on similarity comparisons (i.e Cosine similarity, Euclidean distance) with the query. The top three documents retrieved could then bee "Apple stock: 10", "Pineapple stock: 4" and "Apple juice stock: 10". Finally, the query along with the retrieved documents would then be passed to a LLM for a generated output.

<h2>RAG Applications</h2>
So you are probably now wondering, why do we need RAG when we can simply look through the relevant documents ourselves? This is certainly true, however, what happens if the number of documents to search through is extremely high? For example, there could be over one thousand different items stocks in store to search through. When the number of documents is large, exctracting specific information becomes a challenge, like finding a needle in a haystack. By using RAG, we can efficiently search for specific information, and utilise LLMs to summarise that information for us. Some applications of this might include:

- Enchancing customer support chat bots for company specific queries
- Healthcare for retrieving patient specific records
- Personal Assistants

<h2>Implementing RAG with OpenAI</h2>
Let's do an implementation of RAG using ChatGPT. For this, we'll make a chat bot which can answer specific questions about the Australian Constitution. 

First, lets define our chat bot class. We'll use the LangChain framework to handle our chatbot's memory of the conversation
```python
from dotenv import load_dotenv
from langchain.memory.buffer import ConversationBufferMemory
from langchain_openai import ChatOpenAI

class ConstitutionBot:
    def __init__(self, model_name):
        load_dotenv()
        self.memory = ConversationBufferMemory()
        
        self.llm = ChatOpenAI(
            temperature=0,
            model_name=model_name
        )
        pass
```
We use the load_dotenv function to load our open API key. To use your own API key, make sure you have a '.env' file containing your API key. For example
```python
OPENAI_API_KEY=Your_key_here
   
```

The “Hello world” program is usually the first introduction to any programming language. It looks like this in the C programming language:

```c
/* hello.c */
#import <stdio.h>

int main(int argc, char *argv[]) {
  printf("Hello, world!");
  return 0;
}
```

It demonstrates the minimum amount you need to write a C program. In more modern languages however, this example isn’t as useful anymore. Here’s the same example in Python:

```python
# hello.py
print "Hello, world!"
```

## A better hello world

In today’s world of more succint programming languages, we need a different “hello world” to demonstrate language features better. Here’s what I propose:

```
// hello.js
function getGreeting (name) {
  return `Hello, ${name}!`
}

const message = getGreeting('world')
console.log(message)
```

This simple example demonstrates a few more things than printing strings:

- How to write a function with an argument
- Returning values from functions
- How to use variables
- The naming convention for functions (camelCase versus snake_case)
- String concatenation
- Comments